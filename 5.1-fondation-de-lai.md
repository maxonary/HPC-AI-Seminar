# The foundation of AI - le fondation de l'ai
## A program that can sense, reason, act, and adapt.

![Dr. Mourad Bouache](5-images/dr-mourad-bouache.png)

## NLP - Natural Language Processing - Sentiment Analysis
- Fine-tuning with SST-2 dataset, consisting of movie reviews with positive and negative sentiment labels (reviews).
- Goal: Build a model that can predict the sentiment of new reviews.

## AI will transform 
**Healthcare** - **Finance** - **Transportation** - **Education** - **Entertainment**
- **Retail**
- **Agriculture**
- **Manufacturing**
- **Energy**
- **Environment**
- **Security**
- **Government**
- **Consumer**

## The Layers of AI
![The Layers of AI](5-images/layers-of-ai.png)

The **limits of** moving from Machine Learning to **Deep Learning** are:
1. **Data Size and Quality**
2. **Computing Power and Speed**
2. **Neural Network Algorithms**

#### Example for Machine Learning at Yahoo
Find which email is spam and which is not.
The programs learn from repeatedly seeing data and not by being programmed.

## Machine Learning Terminology
- **Feature** - A characteristic of the data that can be used to predict the target.
- **Target**
- **Label** - The target that the model is trying to predict.

Two types of Machine Learning:
1. **Supervised Learning** - The model is trained on labeled data.
2. **Unsupervised Learning** - The model is trained on unlabeled data.

Machine Learning vs Neural Networks:
![Machine Learning vs Neural Networks](5-images/ml-vs-nn.png)

## End-to-End Computing for AI
Data Center <-> Gateway <-> Edge <-> Endpoint


CPU vs GPU:
![CPU vs GPU](https://www.researchgate.net/publication/270222593/figure/fig1/AS:295022265159684@1447350197467/CPU-vs-GPU-Architecture.png)

GPUs are specialised processors - CPUs are the main computation engines.

The frequency on the CPU is higher than the GPU, but the number of cores on the GPU is higher.

## LLM
A transformer model that was trained on a large amount of data.

## Transformer Architecture
- **Encoder** - Takes the input and converts it into a hidden representation.
- **Decoder** - Takes the hidden representation and converts it into the output.

## Fine Tuning 
- Adapting a pre-trained model to a new task.
- Goal of avoiding training from scratch.
- Gives more accuracy. 

## RAG - Retrieval Augmented Generation
Gives more **context** to the model by providing relevant documents.
Combines the best of both worlds:
- **Retrieval** - Search for relevant information
- **Generation** - Create new information
Retrievs relevant documents, uses the information to generate an answer.
**Advantages**: Less prone to hallucination, grounded in facts.
**Applications**: Chatbots, Question Answering, content summarisation.
__Key is to have a very good summarisation model.__

## RAFT - Robustness from Adversarial Fine-Tuning

## LangChain
A framework for building large language models.